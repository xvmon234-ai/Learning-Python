
# 📈 **[Day 5] Pandas 데이터 정제: 결측치 및 중복값 처리 학습 보고서 및 문제**

### **개요**

본 보고서는 삼일회계법인 Digital 전형 지원을 위한 GitHub 학습 증빙 자료의 일환으로, Pandas 라이브러리를 활용한 데이터 정제 과정 중 **결측치(Missing Values)** 및 **중복값(Duplicate Values)** 처리 학습 내용을 정리하고 관련 연습 문제를 포함합니다. 데이터 분석 및 모델링 전 필수적인 전처리 단계를 이해하고 실제 데이터를 다루는 역량을 보여주는 데 중점을 둡니다.

-----

### **학습 목표**

  * 데이터 내에 존재하는 결측값(NaN, None) 및 중복값을 효율적으로 식별하고 처리하는 기법 숙달.
  * 정확하고 신뢰할 수 있는 데이터 분석 결과를 도출하기 위한 견고한 데이터 전처리 기반 마련.
  * Pandas `isnull()`, `dropna()`, `fillna()`, `duplicated()`, `drop_duplicates()` 등 핵심 메서드의 활용 능력 함양.

-----

### **주요 학습 내용 및 성과**

#### **1. 결측치(Missing Values)의 개념 및 확인**

**이론**

  * 🌟🌟🌟 **결측치(NaN)의 개념 및 중요성**: 데이터 수집 오류, 정보 부재 등으로 인해 발생하는 누락된 값(`np.nan`, `None`, `NaT` 등)을 의미합니다. 통계 왜곡 및 모델 학습 방해를 야기하므로 적절한 처리가 필수적입니다.
  * 🌟🌟🌟 **`df.isnull()` / `df.isna()`**: DataFrame 내의 모든 요소에 대해 결측치 여부를 `True`/`False`로 반환하는 메서드입니다. (두 메서드는 동일 기능)
  * 🌟🌟🌟 **`df.isnull().sum()`**: 각 컬럼별 결측치의 개수를 Series 형태로 반환하여 결측치 현황을 빠르게 파악할 수 있는 **가장 유용한 방법**입니다.
  * 🌟 **`df.isnull().sum().sum()`**: 데이터프레임 전체의 총 결측치 개수를 반환합니다.

**암기할 사항**

  * 🌟🌟🌟 결측치 확인의 **가장 기본이자 필수**: **`df.isnull().sum()`**
  * 🌟 `isnull()`은 불리언 값(True/False)을 반환하며, `.sum()`과 결합하여 개수를 세거나, **불리언 인덱싱**을 통해 결측치가 있는 행을 필터링할 수 있습니다.

**❗ 학습 과정 피드백**

  * **Numpy import 이유**: Pandas는 결측치를 표현할 때 `np.nan`을 사용하므로, Pandas와 함께 Numpy를 import하는 것이 일반적입니다.
  * **불리언 인덱싱 오류 개선**: `df[df["col1"].isnull().sum()]`와 같이 `.sum()`을 포함하면 오류가 발생합니다. `df[df["col1"].isnull()]`처럼 `isnull()`이 반환하는 **불리언 Series 자체**를 사용해야 올바른 필터링이 가능합니다.

-----

#### **✍️ 문제 1. 데이터프레임 생성 및 결측치 확인 (난이도: 하)**

  * **관련 주제**: 5.1 / 결측치(Missing Values)의 개념 및 확인
  * **요구사항**:
    다음 `sample_data` 데이터프레임을 생성하고, 각 컬럼별 결측치 개수를 출력하여 데이터의 누락된 값 현황을 파악하세요.
    ```python
    import pandas as pd
    import numpy as np
    data = {'A': [1, 2, np.nan, 4, 5],
            'B': [np.nan, 6, 7, np.nan, 9],
            'C': ['apple', 'banana', 'cherry', 'date', np.nan],
            'D': [10, 20, 30, 40, 50]}
    sample_data = pd.DataFrame(data)
    ```
  * **학습 목표**: `df.isnull().sum()`을 활용하여 데이터프레임의 결측치 현황을 효율적으로 파악하는 기본기를 다집니다.
  * **가이드**: `df.isnull()` 메서드를 사용한 후, 그 결과를 `sum()` 메서드와 연결하여 각 컬럼의 결측치 개수를 집계하세요.

[▶️ **정답 코드 보러가기**](./solutions/day5_isnull_sum.py)


[📚 **추가 학습 보러가기**](./further_study/method_attribute.py)



-----

#### **2. 결측치(Missing Values) 처리: 삭제**

**이론**

  * 🌟🌟🌟 **`df.dropna()`**: 결측치가 포함된 행 또는 열을 삭제하는 메서드입니다.
      * `axis=0` (기본값): 행(row) 삭제. `axis=1`: 열(column) 삭제.
      * `how='any'` (하나라도 결측치 존재 시 삭제, 기본값) 또는 `how='all'` (해당 행/열의 모든 값이 결측치일 때만 삭제).
      * `subset`: 특정 컬럼(들)을 기준으로 결측치 여부를 확인하여 삭제 범위를 제한합니다.

**암기할 사항**

  * 🌟 **`dropna()`의 `axis`, `how`, `subset`, `inplace` 옵션의 의미와 활용법**을 정확히 이해해야 합니다.
  * 🌟 `dropna()`는 원본을 변경하지 않으므로, 결과를 새로운 변수에 할당하거나 \*\*`inplace=True`\*\*를 사용해야 합니다.

**❗ 학습 과정 피드백**

  * **`dropna()`와 `subset` 사용 시 `axis` 관계**: `subset` 매개변수를 지정하면 `dropna()`는 기본적으로 행(row)을 삭제하는 `axis=0`으로 동작합니다. 따라서 `axis`를 명시적으로 지정하지 않아도 됩니다.

-----

#### **✍️ 문제 2. 모든 결측치가 있는 행/열 삭제 (난이도: 하)**

  * **관련 주제**: 5.2 / 결측치(Missing Values) 처리: 삭제
  * **요구사항**:
    문제 1에서 사용한 `sample_data` 데이터프레임을 복사한 후, 다음 두 가지 작업을 수행하고 각각의 결과를 출력하세요.
    1.  모든 컬럼의 값이 결측치인 **행**을 제거하세요.
    2.  모든 행의 값이 결측치인 **열**을 제거하세요. (만약 해당 열이 없다면, 없다는 메시지를 출력해도 좋습니다.)
  * **학습 목표**: `df.dropna()` 메서드의 `how` 및 `axis` 매개변수를 사용하여 특정 조건(모든 값이 결측치)에 해당하는 행과 열을 삭제하는 방법을 익힙니다.
  * **가이드**:
      * 원본 데이터프레임의 손상을 방지하기 위해 `df.copy()`를 사용하여 작업할 복사본을 만드세요.
      * `how='all'`과 `axis` 매개변수를 조합하여 문제를 해결하세요.


[▶️ **정답 코드 보러가기**](./solutions/day5_copy_dropna.py)


[📚 **추가 학습 보러가기**](./further_study/axis_row_col.py)



#### **✍️ 문제 3. 특정 컬럼 기준 결측치 포함 행 삭제 (난이도: 중)**

  * **관련 주제**: 5.2 / 결측치(Missing Values) 처리: 삭제
  * **요구사항**:
    문제 1에서 사용한 `sample_data` 데이터프레임을 복사한 후, 'A' 컬럼 또는 'C' 컬럼 중 **하나라도** 결측치가 있는 행을 제거하고 결과를 출력하세요.
  * **학습 목표**: `df.dropna()` 메서드의 `subset` 매개변수를 활용하여 특정 컬럼들만을 기준으로 결측치를 검사하고 해당하는 행을 삭제하는 방법을 숙달합니다.
  * **가이드**:
      * `df.copy()`를 사용하여 작업할 복사본을 만드세요.
      * `subset` 매개변수에 리스트 형태로 기준 컬럼들을 전달하고, `how` 매개변수의 기본값(`'any'`)을 활용하세요.

[▶️ **정답 코드 보러가기**](./solutions/day5_dropna_subset.py)


[📚 **추가 학습 보러가기**](./further_study/dropna_subset.py)

-----

#### **3. 결측치(Missing Values) 처리: 대체**

**이론**

  * 🌟🌟🌟 **`df.fillna()`**: 결측치를 특정 값으로 대체하는 가장 일반적이고 강력한 메서드입니다.
      * `value`: 상수(예: `0`, `'Unknown'`), 통계량(예: `df['col'].mean()`, `df['col'].median()`), 문자열 등으로 채웁니다.
      * `method`: `'ffill'` (forward fill, 이전 값으로 채우기) 또는 `'bfill'` (backward fill, 다음 값으로 채우기)과 같이 논리적으로 채울 때 사용합니다.

**암기할 사항**

  * 🌟🌟 **`fillna()`는 결측치를 처리하는 가장 일반적이고 강력한 방법**입니다.
  * 🌟 결측치를 채울 값은 데이터의 특성과 분석 목표에 따라 **신중하게 선택**해야 합니다 (평균/중앙값, 최빈값, `ffill`/`bfill` 등).
  * 🌟 \*\*`ffill`\*\*은 바로 이전(위쪽) 유효한 값으로, \*\*`bfill`\*\*은 바로 다음(아래쪽) 유효한 값으로 결측치를 채웁니다.

**❗ 학습 과정 피드백**

  * **평균값 대체 시 대상 컬럼 일치**: 특정 컬럼의 결측치를 채울 때는 해당 컬럼의 평균을 사용해야 합니다. 다른 컬럼의 평균을 사용하면 데이터의 통계적 특성이 왜곡될 수 있습니다.
  * **`fillna()` 결과의 적용**: `fillna()`는 새로운 Series/DataFrame을 반환하므로, `df['col'] = df['col'].fillna(value)`와 같이 원본 컬럼에 재할당하거나 새로운 변수에 저장해야 변경 사항이 반영됩니다.

-----

#### **✍️ 문제 4. 결측치 유형별 대체 (난이도: 중)**

  * **관련 주제**: 5.3 / 결측치(Missing Values) 처리: 대체
  * **요구사항**:
    문제 1에서 사용한 `sample_data` 데이터프레임을 다시 초기화(재생성)한 후, 다음 지시에 따라 결측치를 대체하고 각각의 결과를 출력하세요.
    1.  'A' 컬럼의 결측치를 해당 컬럼의 **평균값**으로 채우세요.
    2.  'B' 컬럼의 결측치를 바로 **이전(상위) 행의 유효한 값**으로 채우세요. (첫 행이 NaN일 경우 그대로 남을 수 있음을 고려하세요.)
    3.  'C' 컬럼의 결측치를 문자열 \*\*`'MISSING'`\*\*으로 채우세요.
  * **학습 목표**: `df.fillna()` 메서드의 다양한 활용법(상수, 통계량, `ffill`)을 익히고, 각 컬럼의 데이터 특성에 맞는 결측치 대체 전략을 적용하는 능력을 기릅니다.
  * **가이드**:
      * 각 작업 단계마다 `sample_data`를 **재생성**하거나 `df.copy()`를 사용하여 이전 작업이 다음 작업에 영향을 주지 않도록 하세요.
      * `df['컬럼'].mean()`으로 평균값을 구하거나, `method='ffill'`을 사용하세요. `inplace=True` 또는 재할당을 통해 변경사항을 적용하세요.


[▶️ **정답 코드 보러가기**](./solutions/day5_fillna.py)


[📚 **추가 학습 보러가기**](./further_study/fillna_dtype.py)


#### **✍️ 문제 5. 특정 그룹 내 결측치 조건부 대체 (난이도: 상)**

  * **관련 주제**: 5.3 / 결측치(Missing Values) 처리: 대체 (Groupby 응용)
  * **요구사항**:
    다음 `sales_data` 데이터프레임을 생성하세요.
    ```python
    import pandas as pd
    import numpy as np
    data = {'Region': ['East', 'West', 'East', 'West', 'East', 'East'],
            'Product': ['A', 'B', 'A', 'C', 'B', 'A'],
            'Sales': [100, 150, np.nan, 200, 120, np.nan]}
    sales_data = pd.DataFrame(data)
    ```
    'Region' 컬럼을 기준으로 그룹화하여, 각 지역별 `Sales` 컬럼의 결측치를 해당 **지역의 평균 Sales 값**으로 대체하고 결과를 출력하세요.
  * **학습 목표**: `df.groupby()`와 `df.fillna()`를 조합하여 그룹별 통계량을 기반으로 결측치를 대체하는 고급 기법을 학습합니다. 이는 실제 데이터에서 매우 유용한 접근 방식입니다.
  * **가이드**:
      * `sales_data.groupby('Region')['Sales'].transform(lambda x: x.fillna(x.mean()))`과 같은 `transform` 메서드를 활용하는 것이 효과적입니다. `groupby()` 후 `mean()`을 구하고 이를 `fillna()`에 전달하는 방법을 고려해 보세요.
      * 원본 데이터프레임을 직접 수정하려면 재할당 또는 `inplace=True`를 사용하세요.


[▶️ **정답 코드 보러가기**](./solutions/day5_gruopby_transform.py)


[📚 **추가 학습 보러가기**](./further_study/groupby_transform_apply_lamda.py)


-----

#### **4. 중복값(Duplicate Values)의 개념 및 처리**

**이론**

  * 🌟🌟 **`df.duplicated()`**: 각 행이 중복인지 여부를 `True`/`False`로 반환하는 Series를 생성합니다.
  * 🌟🌟 **`df.drop_duplicates()`**: 중복된 행을 실제로 제거하여 유일한 행만 남기는 메서드입니다.
  * `subset`: 특정 컬럼을 기준으로 중복 여부를 확인합니다.
  * `keep`: `'first'` (기본값, 첫 번째 중복 유지), `'last'` (마지막 중복 유지), `False` (모든 중복을 `True`로 표시하거나, `drop_duplicates` 시 모두 제거) 옵션이 있습니다.

**암기할 사항**

  * 🌟 **`duplicated()`는 중복 여부만 확인**하고, **`drop_duplicates()`는 실제로 중복된 행을 제거**합니다.
  * 🌟 `subset`과 `keep` 옵션을 잘 활용하여 원하는 중복 처리 방식을 구현해야 합니다.
  * 🌟 **`sort_values()`와 `drop_duplicates()`를 함께 사용**하면 특정 기준(예: 가장 큰 값)을 가진 중복 행만 남기는 고급 기법을 효과적으로 구현할 수 있습니다.

**❗ 학습 과정 피드백**

  * **`drop_duplicates()`의 `subset` 활용**: `subset`에 특정 컬럼 리스트를 전달하여 해당 컬럼들의 조합이 중복될 경우에만 중복으로 간주하고 제거할 수 있습니다. 이는 복합적인 중복 기준을 설정할 때 유용합니다.

-----

#### **✍️ 문제 6. 다중 조건 중복 제거 및 특정 값 유지 (난이도: 상)**

  * **관련 주제**: 5.4 / 중복값(Duplicate Values)의 개념 및 처리
  * **요구사항**:
    다음 `transaction_logs` 데이터프레임을 생성하세요.
    ```python
    import pandas as pd
    data = {'UserID': [1, 2, 1, 3, 2, 1],
            'Action': ['login', 'view', 'login', 'purchase', 'login', 'view'],
            'Timestamp': ['2023-01-01 10:00', '2023-01-01 10:05', '2023-01-01 10:01', '2023-01-02 11:00', '2023-01-01 10:06', '2023-01-01 10:02'],
            'Value': [10, 5, 12, 100, 8, 6]}
    transaction_logs = pd.DataFrame(data)
    ```
    'UserID'와 'Action'이 동일한 경우를 중복으로 간주하되, 해당 중복 그룹 내에서 'Timestamp'가 **가장 빠른(오래된)** 행만 남기고 나머지를 제거한 결과를 출력하세요.
  * **학습 목표**: `df.sort_values()`와 `df.drop_duplicates()`를 조합하여 다중 컬럼을 기준으로 중복을 정의하고, 특정 컬럼의 값(최소/최대 등)에 따라 중복된 행 중 어떤 것을 유지할지 결정하는 고급 중복 처리 전략을 학습합니다.
  * **가이드**:
      * `Timestamp` 컬럼을 먼저 정렬해야 합니다. 가장 빠른(오래된) 시간을 기준으로 하려면 `ascending=True`로 정렬하세요.
      * 정렬된 데이터프레임에 `drop_duplicates()`를 적용하되, `subset`에 기준 컬럼들을 지정하고 `keep='first'` (기본값)를 활용하여 정렬된 순서의 첫 번째(가장 오래된) 값을 유지하세요.


[▶️ **정답 코드 보러가기**](./solutions/day5_duplicates.py)


[📚 **추가 학습 보러가기**](./further_study/duplicates.py)



-----

### **💡 학습 과정에서 얻은 인사이트 및 해결 경험 (종합)**

Day 5 학습은 데이터 분석의 '기본기'를 다지는 중요한 과정이었습니다. 특히 다음과 같은 점들을 깊이 있게 이해하고 숙련하는 기회가 되었습니다.

  * **Pandas 메서드의 유연성**: `isnull()`, `dropna()`, `fillna()`, `duplicated()`, `drop_duplicates()` 등 Pandas의 다양한 메서드들이 `axis`, `how`, `subset`, `method`, `keep` 등의 풍부한 매개변수를 통해 사용자의 다양한 데이터 처리 요구사항을 충족시킬 수 있음을 체감했습니다. 각 매개변수가 데이터 처리 결과에 미치는 영향을 명확히 이해하게 되었습니다.
  * **원본 데이터 보존과 변경**: `inplace=True`의 중요성과 함께, 메서드 체이닝(`df.sort_values().drop_duplicates()`) 또는 명시적 재할당(`df['col'] = df['col'].fillna(value)`)을 통해 원본 데이터를 효율적으로 관리하는 방법을 익혔습니다.
  * **문제 해결을 위한 조합**: 단순히 메서드를 아는 것을 넘어, `sort_values()`와 `drop_duplicates()`를 조합하여 '가장 큰 Amount를 가진 중복 행만 남기는' 복합적인 문제처럼, 여러 기능을 유기적으로 연결하여 실제 데이터 분석 과제를 해결하는 사고력을 길렀습니다.
  * **정확한 이해의 중요성**: `isnull().sum()`과 `isnull()`의 차이, 특정 컬럼의 평균으로 대체 시 해당 컬럼의 평균을 사용해야 하는 이유 등, 작은 디테일들이 코드의 정확성과 결과의 신뢰성에 큰 영향을 미침을 인지했습니다. 이는 향후 데이터 전처리 시 발생할 수 있는 잠재적 오류를 예방하는 데 큰 도움이 될 것입니다.

